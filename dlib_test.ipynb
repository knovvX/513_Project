{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import kagglehub\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Download the dataset\n",
    "print(\"Downloading iBUG 300-W dataset...\")\n",
    "path = kagglehub.dataset_download(\"toxicloser/ibug-300w-large-face-landmark-dataset\")\n",
    "print(f\"Dataset downloaded to: {path}\")\n",
    "\n",
    "# Step 2: Set up dlib face detector and landmark predictor\n",
    "print(\"Setting up dlib models...\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"  # Make sure you have this file\n",
    "\n",
    "# Check if predictor file exists, if not download it\n",
    "if not os.path.exists(predictor_path):\n",
    "    print(\"Downloading dlib's 68 point facial landmark predictor...\")\n",
    "    # You can download it from dlib's GitHub or website\n",
    "    # For this example, please download it manually from:\n",
    "    # http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "    print(f\"Please download the predictor file from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "    print(f\"Extract it and place it in the current directory as {predictor_path}\")\n",
    "    exit(1)\n",
    "\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Function to convert dlib shape to numpy array\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    return coords\n",
    "\n",
    "# Function to read the iBUG landmark format\n",
    "def read_landmarks(landmark_file):\n",
    "    with open(landmark_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Skip the header and version info\n",
    "    lines = [line.strip() for line in lines if not line.startswith('version') and line.strip()]\n",
    "    \n",
    "    # Extract number of points\n",
    "    n_points = int(lines[0].split()[1])\n",
    "    \n",
    "    # Read landmark coordinates\n",
    "    landmarks = np.zeros((n_points, 2))\n",
    "    for i in range(n_points):\n",
    "        x, y = map(float, lines[i+1].split())\n",
    "        landmarks[i] = [x, y]\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "# Function to calculate the normalized mean error (NME)\n",
    "def calculate_nme(pred_landmarks, gt_landmarks, norm_factor):\n",
    "    \"\"\"\n",
    "    Calculate the Normalized Mean Error\n",
    "    norm_factor: usually the inter-ocular distance (distance between eyes)\n",
    "    \"\"\"\n",
    "    assert pred_landmarks.shape == gt_landmarks.shape, \"Shapes don't match\"\n",
    "    \n",
    "    # Calculate Euclidean distance for each landmark\n",
    "    error = np.sqrt(np.sum((pred_landmarks - gt_landmarks)**2, axis=1))\n",
    "    \n",
    "    # Normalize by the normalization factor\n",
    "    normalized_error = error / norm_factor\n",
    "    \n",
    "    # Return mean normalized error\n",
    "    return np.mean(normalized_error)\n",
    "\n",
    "# Function to calculate inter-ocular distance (distance between eyes)\n",
    "def calculate_inter_ocular_distance(landmarks):\n",
    "    # For 68-point landmarks, points 36-41 are right eye, 42-47 are left eye\n",
    "    right_eye_center = np.mean(landmarks[36:42], axis=0)\n",
    "    left_eye_center = np.mean(landmarks[42:48], axis=0)\n",
    "    return np.linalg.norm(right_eye_center - left_eye_center)\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_dlib_accuracy():\n",
    "    # Find all images and their corresponding landmark files\n",
    "    dataset_dir = path\n",
    "    subsets = ['ibug', 'afw', 'helen/trainset', 'helen/testset', 'lfpw/trainset', 'lfpw/testset']\n",
    "    \n",
    "    all_errors = []\n",
    "    subset_errors = {subset: [] for subset in subsets}\n",
    "    \n",
    "    total_images = 0\n",
    "    processed_images = 0\n",
    "    failed_detections = 0\n",
    "    \n",
    "    for subset in subsets:\n",
    "        subset_path = os.path.join(dataset_dir, subset)\n",
    "        if not os.path.exists(subset_path):\n",
    "            print(f\"Warning: Path {subset_path} does not exist. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        image_files = glob.glob(os.path.join(subset_path, '*.jpg')) + glob.glob(os.path.join(subset_path, '*.png'))\n",
    "        total_images += len(image_files)\n",
    "        \n",
    "        print(f\"\\nProcessing {subset}: {len(image_files)} images\")\n",
    "        \n",
    "        for img_path in tqdm(image_files):\n",
    "            # Find corresponding landmark file\n",
    "            pts_path = os.path.splitext(img_path)[0] + '.pts'\n",
    "            if not os.path.exists(pts_path):\n",
    "                print(f\"Warning: No landmark file for {img_path}. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            # Read image\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            # Read ground truth landmarks\n",
    "            gt_landmarks = read_landmarks(pts_path)\n",
    "            \n",
    "            # Run dlib detector\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            rects = detector(gray, 1)\n",
    "            \n",
    "            if len(rects) == 0:\n",
    "                print(f\"Warning: No face detected in {img_path}. Skipping.\")\n",
    "                failed_detections += 1\n",
    "                continue\n",
    "                \n",
    "            # Use the first detected face\n",
    "            rect = rects[0]\n",
    "            \n",
    "            # Apply landmark predictor\n",
    "            shape = predictor(gray, rect)\n",
    "            pred_landmarks = shape_to_np(shape)\n",
    "            \n",
    "            # Calculate normalization factor (inter-ocular distance)\n",
    "            iod = calculate_inter_ocular_distance(gt_landmarks)\n",
    "            \n",
    "            # Calculate error\n",
    "            error = calculate_nme(pred_landmarks, gt_landmarks, iod)\n",
    "            all_errors.append(error)\n",
    "            subset_errors[subset].append(error)\n",
    "            \n",
    "            processed_images += 1\n",
    "    \n",
    "    # Print overall statistics\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Processed images: {processed_images}\")\n",
    "    print(f\"Failed detections: {failed_detections}\")\n",
    "    print(f\"Detection rate: {processed_images/total_images*100:.2f}%\")\n",
    "    \n",
    "    if processed_images > 0:\n",
    "        print(f\"\\nOverall Mean NME: {np.mean(all_errors)*100:.4f}%\")\n",
    "        print(f\"Overall Median NME: {np.median(all_errors)*100:.4f}%\")\n",
    "        \n",
    "        # Print per-subset statistics\n",
    "        print(\"\\nPer-subset NME:\")\n",
    "        for subset in subsets:\n",
    "            if subset_errors[subset]:\n",
    "                print(f\"{subset}: {np.mean(subset_errors[subset])*100:.4f}%\")\n",
    "        \n",
    "        # Plot error distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(np.array(all_errors)*100, bins=50, alpha=0.7)\n",
    "        plt.xlabel('Normalized Mean Error (%)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Landmark Detection Errors')\n",
    "        plt.axvline(np.mean(all_errors)*100, color='r', linestyle='dashed', linewidth=1, label=f'Mean: {np.mean(all_errors)*100:.4f}%')\n",
    "        plt.axvline(np.median(all_errors)*100, color='g', linestyle='dashed', linewidth=1, label=f'Median: {np.median(all_errors)*100:.4f}%')\n",
    "        plt.legend()\n",
    "        plt.savefig('dlib_error_distribution.png')\n",
    "        print(\"Error distribution plot saved as 'dlib_error_distribution.png'\")\n",
    "        \n",
    "        # Plot some example images with landmarks\n",
    "        plot_sample_results(dataset_dir, subsets)\n",
    "\n",
    "def plot_sample_results(dataset_dir, subsets, num_samples=5):\n",
    "    \"\"\"Plot some sample results to visualize the landmark detection\"\"\"\n",
    "    plt.figure(figsize=(15, 3*num_samples))\n",
    "    \n",
    "    sample_count = 0\n",
    "    for subset in subsets:\n",
    "        subset_path = os.path.join(dataset_dir, subset)\n",
    "        if not os.path.exists(subset_path):\n",
    "            continue\n",
    "            \n",
    "        image_files = glob.glob(os.path.join(subset_path, '*.jpg')) + glob.glob(os.path.join(subset_path, '*.png'))\n",
    "        if not image_files:\n",
    "            continue\n",
    "            \n",
    "        # Take a random sample\n",
    "        import random\n",
    "        sample_files = random.sample(image_files, min(num_samples//len(subsets) + 1, len(image_files)))\n",
    "        \n",
    "        for img_path in sample_files:\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            # Find corresponding landmark file\n",
    "            pts_path = os.path.splitext(img_path)[0] + '.pts'\n",
    "            if not os.path.exists(pts_path):\n",
    "                continue\n",
    "                \n",
    "            # Read image\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "                \n",
    "            # Read ground truth landmarks\n",
    "            gt_landmarks = read_landmarks(pts_path)\n",
    "            \n",
    "            # Run dlib detector\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            rects = detector(gray, 1)\n",
    "            \n",
    "            if len(rects) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Use the first detected face\n",
    "            rect = rects[0]\n",
    "            \n",
    "            # Apply landmark predictor\n",
    "            shape = predictor(gray, rect)\n",
    "            pred_landmarks = shape_to_np(shape)\n",
    "            \n",
    "            # Calculate normalization factor (inter-ocular distance)\n",
    "            iod = calculate_inter_ocular_distance(gt_landmarks)\n",
    "            \n",
    "            # Calculate error\n",
    "            error = calculate_nme(pred_landmarks, gt_landmarks, iod)\n",
    "            \n",
    "            # Plot\n",
    "            plt.subplot(num_samples, 3, sample_count*3 + 1)\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f'Original Image\\n{os.path.basename(img_path)}')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(num_samples, 3, sample_count*3 + 2)\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            for i in range(68):\n",
    "                plt.plot(gt_landmarks[i, 0], gt_landmarks[i, 1], 'go', markersize=3)\n",
    "            plt.title('Ground Truth Landmarks')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(num_samples, 3, sample_count*3 + 3)\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            for i in range(68):\n",
    "                plt.plot(pred_landmarks[i, 0], pred_landmarks[i, 1], 'ro', markersize=3)\n",
    "            plt.title(f'Dlib Predictions\\nNME: {error*100:.2f}%')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            sample_count += 1\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('landmark_detection_samples.png')\n",
    "    print(\"Sample landmark detection results saved as 'landmark_detection_samples.png'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dlib_accuracy()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
